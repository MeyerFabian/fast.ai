{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "     \n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out device 0 if cuda is a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if(torch.cuda.is_available() & torch.cuda.device_count()>0):\n",
    "  print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.52'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python        : 3.7.3\n",
      "fastai        : 1.0.52\n",
      "fastprogress  : 0.1.21\n",
      "torch         : 1.1.0\n",
      "torch cuda    : 10.0 / is available\n",
      "torch cudnn   : 7401 / is enabled\n",
      "\n",
      "=== Hardware === \n",
      "torch devices : 1\n",
      "  - gpu0      : GeForce GTX 970\n",
      "\n",
      "=== Environment === \n",
      "platform      : Windows-10-10.0.17134-SP0\n",
      "conda env     : python37\n",
      "python        : D:\\Software\\Miniconda\\envs\\python37\\python.exe\n",
      "sys.path      : C:\\Users\\fabia\\ProgramingEnv\\Projects\\fast.ai\\fast.ai\n",
      "D:\\Software\\Miniconda\\envs\\python37\\python37.zip\n",
      "D:\\Software\\Miniconda\\envs\\python37\\DLLs\n",
      "D:\\Software\\Miniconda\\envs\\python37\\lib\n",
      "D:\\Software\\Miniconda\\envs\\python37\n",
      "\n",
      "D:\\Software\\Miniconda\\envs\\python37\\lib\\site-packages\n",
      "D:\\Software\\Miniconda\\envs\\python37\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\fabia\\ProgramingEnv\\.ipython\n",
      "no nvidia-smi is found\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fastai.utils;\n",
    "fastai.utils.show_install()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running performance checks.\n",
      "\n",
      "*** libjpeg-turbo status\n",
      "❓ libjpeg-turbo's status can't be derived - need Pillow(-SIMD)? >= 5.4.0 to tell, current version 5.3.0.post0\n",
      "\n",
      "*** Pillow-SIMD status\n",
      "✔ Running Pillow-SIMD 5.3.0.post0\n",
      "\n",
      "*** CUDA status\n",
      "✔ Running the latest CUDA 10.0 with NVIDIA driver 430.86\n",
      "\n",
      "Refer to https://docs.fast.ai/performance.html to make sense out of these checks and suggestions.\n"
     ]
    }
   ],
   "source": [
    "fastai.utils.check_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU RAM Free: 3444MB | Used: 652MB | Util  16% | Total 4096MB\n"
     ]
    }
   ],
   "source": [
    "gpu = GPUs[0]\n",
    "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1, 1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
